import torch
from pathlib import Path
import copy
import time
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import pdb
import skimage
from distutils.version import LooseVersion
from skimage.transform import resize as sk_resize

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def resize(image, output_shape, order=1, mode='constant', cval=0, clip=True,
           preserve_range=False, anti_aliasing=False, anti_aliasing_sigma=None):
    """A wrapper for Scikit-Image resize().
    Scikit-Image generates warnings on every call to resize() if it doesn't
    receive the right parameters. The right parameters depend on the version
    of skimage. This solves the problem by using different parameters per
    version. And it provides a central place to control resizing defaults.
    """
    if LooseVersion(skimage.__version__) >= LooseVersion("0.14"):
        # New in 0.14: anti_aliasing. Default it to False for backward
        # compatibility with skimage 0.13.
        return skimage.transform.resize(
            image, output_shape,
            order=order, mode=mode, cval=cval, clip=clip,
            preserve_range=preserve_range, anti_aliasing=anti_aliasing,
            anti_aliasing_sigma=anti_aliasing_sigma)
    else:
        return skimage.transform.resize(
            image, output_shape,
            order=order, mode=mode, cval=cval, clip=clip,
            preserve_range=preserve_range)


def minimize_mask(bbox, mask, mini_shape):
    """Resize masks to a smaller version to reduce memory load.
    Mini-masks can be resized back to image scale using expand_masks()
    See inspect_data.ipynb notebook for more details.
    """
    mini_mask = np.zeros(mini_shape + (mask.shape[-1],), dtype=bool)
    for i in range(mask.shape[-1]):
        # Pick slice and cast to bool in case load_mask() returned wrong dtype
        m = mask[:, :, i].astype(bool)
        y1, x1, y2, x2 = bbox[i][:4]
        m = m[y1:y2, x1:x2]
        if m.size == 0:
            raise Exception("Invalid bounding box with area of zero")
        # Resize with bilinear interpolation
        m = resize(m, mini_shape)
        mini_mask[:, :, i] = np.around(m).astype(np.bool)
    return mini_mask


def expand_mask(bbox, mini_mask, image_shape):
    """Resizes mini masks back to image size. Reverses the change
    of minimize_mask().
    See inspect_data.ipynb notebook for more details.
    """
    mask = np.zeros((mini_mask.shape[0],) +image_shape[:2] , dtype=bool)
    for i in range(mask.shape[0]):
        m = mini_mask[i, :, :]
        y1, x1, y2, x2 = bbox[i][:4]
        h = y2 - y1
        w = x2 - x1
        # Resize with bilinear interpolation
        m = resize(m, (h, w))
        mask[i, y1:y2, x1:x2] = np.around(m).astype(np.bool)
    return mask


def unmold_mask(mask, bbox, image_shape):
    """Converts a mask generated by the neural network to a format similar
    to its original shape.
    mask: [height, width] of type float. A small, typically 28x28 mask.
    bbox: [y1, x1, y2, x2]. The box to fit the mask in.
    Returns a binary mask with the same size as the original image.
    """
    threshold = 0.5
    y1, x1, y2, x2 = bbox
    mask = resize(mask, (y2 - y1, x2 - x1))
    mask = np.where(mask >= threshold, 1, 0).astype(np.bool)

    # Put the mask in the right location.
    full_mask = np.zeros(image_shape, dtype=np.bool)
    full_mask[y1:y2, x1:x2] = mask
    return full_mask


def model_out_to_unmold(outputs28):
    batch_size = outputs28.size(0)
    outputs28_np = outputs28.detach().cpu().numpy()  # has shape (batch_size, 1, 28, 28)
    outputs28_np = outputs28_np[:, 0, :, :].transpose(1, 2, 0)  # makes it (28, 28, batch_size)

    preds224 = unmold_mask(outputs28_np, [0, 0, 223, 223], (224, 224, batch_size))[np.newaxis, ...]\
        .transpose(3, 0, 1, 2)\
        .astype(np.float32)  # outputs (224,224, batch_size) - insert axis at 0, do another transpose

    return torch.from_numpy(preds224)


def viz_prediction(track_sample, pred, epoch):
    scans, label = track_sample

    scans, label = scans.numpy().transpose((1, 2, 0)), label.numpy()[0][..., np.newaxis]
    pred = pred[0].numpy()[..., np.newaxis]

    scans_stack = np.concatenate([scans, label, pred], axis=-1)

    fig = plt.figure(figsize=(20, 6))

    fig.suptitle('TRACKING Sample')

    for slice_, scan in enumerate(['dwi', 'flair', 't1', 't2', 'label', 'predicted']):
        ax = plt.subplot(1, 6, slice_ + 1)
        show_single_img(scans_stack[:, :, slice_], (scan == 'label' or scan == 'predicted'))
        plt.tight_layout()
        ax.set_title(scan)
        ax.axis('off')

    # plt.show()
    plt.savefig('sample_tracking/'+ str(epoch)+ '.jpg')

def actual_predicted(actual, predicted, save_path):
    fig = plt.figure(figsize=(10,5))

    fig.suptitle('Actual-Predicted')

    ax = plt.subplot(1, 2, 1)
    show_single_img(actual)
    plt.tight_layout()
    ax.set_title('Actual')
    ax.axis('off')

    ax = plt.subplot(1, 2, 2)
    show_single_img(predicted)
    plt.tight_layout()
    ax.set_title('Predicted')
    ax.axis('off')

    # plt.show()

    plt.savefig(save_path)

def show_single_img(image, label=False):
    """Show image"""
    cmap = 'gray'
    if label:
        cmap = 'binary'
    plt.imshow(image, cmap = cmap)


def get_prob_map28(outputs28):
    # based on argmax
    max_prob, pred28_argmax = torch.max(outputs28, dim=1, keepdim=True)  # (batch_size, 1, 28,28)

    # based on prob
    pred28 = outputs28.data
    pred28[:, 0, :, :] = 1 - outputs28[:, 0, :, :]
    one_hot = F.one_hot(pred28_argmax.squeeze()).permute(0, 3, 1, 2).bool()  # (batch_size, 2 classes, 28,28)
    pred28_prob = torch.sum(pred28 * one_hot, dim=1, keepdim=True)  # (batch_size, 1 val, 28, 28)

    # pdb.set_trace()

    return pred28_prob



def dice_loss(input, target):
    smooth = 1.

    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()

    return 1 - ((2. * intersection + smooth) /
                (iflat.sum() + tflat.sum() + smooth))


# borrow functions and modify it from https://github.com/Kaixhin/FCN-semantic-segmentation/blob/master/main.py
# Calculates class intersections over unions
def iou(pred, target):
    ious = []
    n_class = 2
    for cls in range(n_class):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = pred_inds[target_inds].sum()
        union = pred_inds.sum() + target_inds.sum() - intersection
        if union == 0:
            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / max(union, 1))
        # print("cls", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))
    return ious


def pixel_acc(pred, target):
    correct = (pred == target).sum()
    total   = (target == target).sum()
    return correct / total
